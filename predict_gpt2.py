import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# 1ï¸âƒ£ é€‰æ‹© GPU è¿è¡Œ
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âœ… å½“å‰è®¾å¤‡: {device}")

# 2ï¸âƒ£ åŠ è½½è®­ç»ƒå¥½çš„ GPT-2 æ¨¡å‹
model_path = "./bet_gpt2_model"  # ç¡®ä¿è·¯å¾„æ­£ç¡®
model = GPT2LMHeadModel.from_pretrained(model_path).to(device)
tokenizer = GPT2Tokenizer.from_pretrained(model_path)

# 3ï¸âƒ£ èµ”ç‡é¢„æµ‹å‡½æ•°
def predict_winner_odds(odds_list):
    input_text = f"èµ”ç‡åˆ—è¡¨: {odds_list} é¢„æµ‹èƒœåˆ©èµ”ç‡:"
    inputs = tokenizer(input_text, return_tensors="pt").to(device)

    # ç”Ÿæˆæ–‡æœ¬
    output = model.generate(**inputs, max_length=128, num_return_sequences=1)
    prediction = tokenizer.decode(output[0], skip_special_tokens=True)

    print(f"\nğŸ“ GPT ç”Ÿæˆçš„å®Œæ•´è¾“å‡º: {prediction}")

    # è§£æ GPT é¢„æµ‹çš„èµ”ç‡
    predicted_odds = None
    try:
        if "é¢„æµ‹èƒœåˆ©èµ”ç‡:" in prediction:
            predicted_odds = float(prediction.split("é¢„æµ‹èƒœåˆ©èµ”ç‡:")[-1].strip())
        else:
            print("âš  GPT ç”Ÿæˆçš„ç»“æœæ²¡æœ‰åŒ…å«èµ”ç‡ï¼Œå›é€€åˆ°é»˜è®¤ç­–ç•¥ã€‚")
            predicted_odds = odds_list[0]  # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤è¿”å›æœ€ä½èµ”ç‡
    except ValueError:
        print("âš  æ— æ³•è§£æ GPT é¢„æµ‹çš„èµ”ç‡ï¼Œå›é€€åˆ°é»˜è®¤ç­–ç•¥ã€‚")
        predicted_odds = odds_list[0]  # é¿å…å´©æºƒ

    # ğŸ”¥ ç¡®ä¿é¢„æµ‹çš„èµ”ç‡åœ¨è¾“å…¥èµ”ç‡èŒƒå›´å†…
    closest_odds = min(odds_list, key=lambda x: abs(x - predicted_odds))

    print(f"\nğŸ† GPT é¢„æµ‹: {predicted_odds:.2f}ï¼Œæœ€ç»ˆè°ƒæ•´ä¸º: {closest_odds:.2f}")
    return closest_odds

# 4ï¸âƒ£ æµ‹è¯•æ–°çš„èµ”ç‡é¢„æµ‹
#test_odds = [1.5, 2.3, 3.0, 1.8]  # ä½ å¯ä»¥æ¢æˆæ–°çš„èµ”ç‡åˆ—è¡¨
test_odds = [1.86, 2.17]  
predicted_winner_odds = predict_winner_odds(test_odds)

print(f"\nğŸ† æœ€å¯èƒ½çš„èƒœåˆ©èµ”ç‡: {predicted_winner_odds:.2f}")
