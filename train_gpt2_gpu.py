import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset

# 1ï¸âƒ£ é€‰æ‹© GPU è®­ç»ƒ
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âœ… å½“å‰è®¾å¤‡: {device}")

# 2ï¸âƒ£ åŠ è½½ tokenizer å’Œ model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # è§£å†³ padding é—®é¢˜
model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)  # ğŸ”¥ è®© GPT-2 è¿è¡Œåœ¨ GPU

# 3ï¸âƒ£ è¯»å– JSON è®­ç»ƒæ•°æ®
dataset = load_dataset("json", data_files="bet_gpt_training.json", split="train")

# 4ï¸âƒ£ Tokenize æ•°æ®
def tokenize_function(data):
    input_text = f"èµ”ç‡åˆ—è¡¨: {data['odds_list']} é¢„æµ‹èƒœåˆ©èµ”ç‡:"
    output_text = str(data["winner_odds"])
    full_text = input_text + " " + output_text
    return tokenizer(full_text, padding="max_length", truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function)

# 5ï¸âƒ£ å¯ç”¨ GPU è®­ç»ƒ
training_args = TrainingArguments(
    output_dir="./bet_gpt2_model",
    per_device_train_batch_size=16,  # ğŸ”¥ å¢å¤§ batch_sizeï¼Œæé«˜ GPU è®¡ç®—æ•ˆç‡
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    save_steps=500,
    logging_steps=100,
    learning_rate=5e-5,
    weight_decay=0.01,
    save_total_limit=2,
    eval_strategy="no",
    fp16=True,  # ğŸ”¥ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ŒåŠ é€Ÿè®¡ç®—
)

trainer = Trainer(
    model=model,  # ä½¿ç”¨ GPU è®­ç»ƒçš„æ¨¡å‹
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

# 6ï¸âƒ£ è®­ç»ƒæ¨¡å‹
trainer.train()
trainer.save_model("./bet_gpt2_model")
tokenizer.save_pretrained("./bet_gpt2_model")

print("âœ… GPT-2 è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜ï¼")
