from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset

# åŠ è½½ tokenizer å’Œ model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# ğŸ”¥ è§£å†³ padding é—®é¢˜
tokenizer.pad_token = tokenizer.eos_token

# è¯»å– JSON è®­ç»ƒæ•°æ®
dataset = load_dataset("json", data_files="bet_gpt_training.json", split="train")

# Tokenize æ•°æ®
def tokenize_function(data):
    input_text = f"èµ”ç‡åˆ—è¡¨: {data['odds_list']} é¢„æµ‹èƒœåˆ©èµ”ç‡:"
    output_text = str(data["winner_odds"])
    full_text = input_text + " " + output_text
    return tokenizer(full_text, padding="max_length", truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function)

# ğŸ”¥ è§£å†³ `eval_strategy` æŠ¥é”™
training_args = TrainingArguments(
    output_dir="./bet_gpt2_model",
    per_device_train_batch_size=8,
    num_train_epochs=3,
    save_steps=500,
    logging_steps=100,
    learning_rate=5e-5,
    weight_decay=0.01,
    save_total_limit=2,
    eval_strategy="no",  # ğŸ”¥ å…³é—­ evalï¼Œé¿å…æŠ¥é”™
)

trainer = Trainer(
    model=GPT2LMHeadModel.from_pretrained("gpt2"),
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

# è®­ç»ƒæ¨¡å‹
trainer.train()
trainer.save_model("./bet_gpt2_model")
tokenizer.save_pretrained("./bet_gpt2_model")
print("âœ… GPT-2 è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜ï¼")
